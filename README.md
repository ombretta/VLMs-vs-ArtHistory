# Have Large Vision-Language Models Mastered Art History?

## Introduction

This repository accompanies the paper "Have Large Vision-Language Models Mastered Art History?".

The emergence of large Vision-Language Models (VLMs) has recently established new baselines in image classification across multiple domains. However, the performance of VLMs in the specific task of artwork classification, particularly art style classification of paintings — a domain traditionally mastered by art historians — has not been explored yet. Artworks pose a unique challenge compared to natural images due to their inherently complex and diverse structures, characterized by variable compositions and styles. Art historians have long studied the unique aspects of artworks, with style prediction being a crucial component of their discipline. This paper investigates whether large VLMs, which integrate visual and textual data, can effectively predict the art historical attributes of paintings. We conduct an in-depth analysis of four VLMs, namely CLIP, LLaVA, OpenFlamingo, and GPT-4o, focusing on zero-shot classification of art style, author and time period using two public benchmarks of artworks. Additionally, we present ArTest, a well-curated test set of artworks, including pivotal paintings studied by art historians.


## Project Structure
The repository contains the code to reproduce the main results from the paper, as well as the data and results collected in our project and the new ArTest test set, curated by art historians.  

Here is an overview of the project's structure and the purpose of each directory:

```
├── readme.md
└── src
    └── main
        └── java
            └── programming
                ├── Account.java
                ├── App.java
                ├── Employee.java
                ├── EmployeeFactory.java
```


### Directories

- **`src/`**: This directory contains the source code for the project.
  - `main.py`: The main script that runs the application.
  - `utils.py`: Utility functions used across the project.

- **`data/`**: This folder contains datasets or data files used in the project.
  - `input.csv`: The input data for the application.
  - `output.csv`: The output data generated by the application.


## VLMs inference 

The prediction task is specified through the `--prompt` argument (e.g., `--prompt 
"To which art style does this painting belong?"`) and the `--attribute` argument (art_style, author, year). 

### CLIP

Based on the [official CLIP implementation](https://github.com/openai/CLIP). Requirements installation through:

`conda env create -f clip_environment.yml`

Example of usage: Art style prediction on ArTest 

    conda activate clip
    python CLIP.py  --dataset_name ArTest --image_folder_path datasets/ArTest/ --annotation_file datasets/ArTest/ArTest.csv --features_dir datasets/ArTest/CLIP_features/ --prompt the\ art\ style\ of\ the\ painting\ is\  --image_model ViT-B/32 --attribute art_style --n -1


### LLaVA

We utilize the [HuggingFace implementation](https://huggingface.co/llava-hf/llava-1.5-7b-hf), based on the `transformers` library. Requirements installation through:

`conda env create -f llava_environment.yml`

Please note: The code runs only on GPU.

Example of usage: Art style prediction on ArTest 

    conda activate llava
    python LLaVA.py --dataset_name ArTest --image_folder_path datasets/ArTest/ --annotation_file datasets/ArTest/ArTest.csv --prompt "To which art style does this painting belong?" --n -1 --attribute art_style


### OpenFlamingo 

We utilize the [HuggingFace implementation](https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b). Requirements installation through:

`conda env create -f openflamingo_environment.yml`

Please note: The code runs only on GPU.

Example of usage: Art style prediction on ArTest 
    
    conda activate openflamingo 
    python OpenFlamingo.py --dataset_name ArTest --image_folder_path datasets/ArTest/ --annotation_file datasets/ArTest/ArTest.csv --attribute art_style --prompt "The art style of this painting is "



### GPT-4o

Utilizes the [Open API](https://platform.openai.com/docs/overview). Since the code uses the API, it does not require a dedicated conda environment.  
To use the API, you need a OpenAI key. Please create and copy your key in a OpenAI_key.txt file.

Example of usage: Art style prediction on ArTest
    
    python GPT4o.py --dataset_name ArTest --image_folder_path datasets/ArTest/ --annotation_file datasets/ArTest/ArTest.csv --clip_prompt "To which art style does this painting belong?" --n -1 --attribute art_style



### Evaluation



